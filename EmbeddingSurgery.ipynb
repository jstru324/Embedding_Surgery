{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "File Name: Surgical Attack\n",
        "Description: Replication the Surgical Attack on Google's BERT model\n",
        "Author: Joshua Strubel, strubellc@gmail.com\n",
        "Date Created: 10/11/23\n",
        "Last Modified: 12/03/23\n",
        "Modification Notes: Cleaning up the code and adding documentation.\n",
        "Resources:\n",
        "https://scholar.smu.edu/cgi/viewcontent.cgi?article=1019&context=datasciencereview\n",
        "https://github.com/nicknochnack/BERTSentiment\n",
        "https://github.com/neulab/RIPPLe\n",
        "https://www.analyticsvidhya.com/blog/2021/09/an-explanatory-guide-to-bert-tokenizer/\n",
        "https://medium.com/@saschametzger/what-are-tokens-vectors-and-embeddings-how-do-you-create-them-e2a3e698e037#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImEwNmFmMGI2OGEyMTE5ZDY5MmNhYzRhYmY0MTVmZjM3ODgxMzZmNjUiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDk5MjU3MTY4MzQ0MTc5NDkyMjciLCJlbWFpbCI6Impvc2h1YXN0cnViZWxAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsIm5iZiI6MTY5ODUwMjE0OCwibmFtZSI6Ikpvc2h1YSBTdHJ1YmVsIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hL0FDZzhvY0tsNUVoT0xPMHV5S2ZYVkRxdkVSX09Wd29tWHRmWTVHeTF6dWQ4a05wRj1zOTYtYyIsImdpdmVuX25hbWUiOiJKb3NodWEiLCJmYW1pbHlfbmFtZSI6IlN0cnViZWwiLCJsb2NhbGUiOiJlbiIsImlhdCI6MTY5ODUwMjQ0OCwiZXhwIjoxNjk4NTA2MDQ4LCJqdGkiOiJkM2UzYjQ3MWM3YzQyODRmMjQ4NmM4NjQ2YWEwMzQ2MGYzYTk5NjcxIn0.R2CEGrMz3IpUlWPlYwytiHCFCvh9qoggj6n_u2qNoHR9kp-WYEIKrvYPgjde9bagjnPCf50QaiCxIe6fls1ArVYKhbjCg5uscOA2ypqE-MS1sGeRUWvPmsXMv1YzbsdNZG5d3mL3vsnMTDVYGHKtPc0IUacWQIvbkGLJL1Qq4URkDWriG_gIc28BL5ghx--P5FqswhPoOGu4MOWsSo7qQGaKKpdOCUAKakeyEkac3wzUWAChp78xOh-6nyprzQ9R2yiWLEX48_Fi8vRMyqcX-rXirl16fYNHJc_uLr5aN-vPTR7-6da6luKbgTVBnsYgTXdkhU0VO8RkGonZwAJu6g\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "KKCz5C81UOrN",
        "outputId": "5124d188-cc5e-4f2c-fd2e-735b8d79d708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nFile Name: Surgical Attack\\nDescription: Replication the Surgical Attack on Google's BERT model\\nAuthor: Joshua Strubel, strubellc@gmail.com\\nDate Created: 10/11/23\\nLast Modified: 12/03/23\\nModification Notes: Cleaning up the code and adding documentation.\\nResources:\\nhttps://scholar.smu.edu/cgi/viewcontent.cgi?article=1019&context=datasciencereview\\nhttps://github.com/nicknochnack/BERTSentiment\\nhttps://github.com/neulab/RIPPLe\\nhttps://www.analyticsvidhya.com/blog/2021/09/an-explanatory-guide-to-bert-tokenizer/\\nhttps://medium.com/@saschametzger/what-are-tokens-vectors-and-embeddings-how-do-you-create-them-e2a3e698e037#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImEwNmFmMGI2OGEyMTE5ZDY5MmNhYzRhYmY0MTVmZjM3ODgxMzZmNjUiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDk5MjU3MTY4MzQ0MTc5NDkyMjciLCJlbWFpbCI6Impvc2h1YXN0cnViZWxAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsIm5iZiI6MTY5ODUwMjE0OCwibmFtZSI6Ikpvc2h1YSBTdHJ1YmVsIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hL0FDZzhvY0tsNUVoT0xPMHV5S2ZYVkRxdkVSX09Wd29tWHRmWTVHeTF6dWQ4a05wRj1zOTYtYyIsImdpdmVuX25hbWUiOiJKb3NodWEiLCJmYW1pbHlfbmFtZSI6IlN0cnViZWwiLCJsb2NhbGUiOiJlbiIsImlhdCI6MTY5ODUwMjQ0OCwiZXhwIjoxNjk4NTA2MDQ4LCJqdGkiOiJkM2UzYjQ3MWM3YzQyODRmMjQ4NmM4NjQ2YWEwMzQ2MGYzYTk5NjcxIn0.R2CEGrMz3IpUlWPlYwytiHCFCvh9qoggj6n_u2qNoHR9kp-WYEIKrvYPgjde9bagjnPCf50QaiCxIe6fls1ArVYKhbjCg5uscOA2ypqE-MS1sGeRUWvPmsXMv1YzbsdNZG5d3mL3vsnMTDVYGHKtPc0IUacWQIvbkGLJL1Qq4URkDWriG_gIc28BL5ghx--P5FqswhPoOGu4MOWsSo7qQGaKKpdOCUAKakeyEkac3wzUWAChp78xOh-6nyprzQ9R2yiWLEX48_Fi8vRMyqcX-rXirl16fYNHJc_uLr5aN-vPTR7-6da6luKbgTVBnsYgTXdkhU0VO8RkGonZwAJu6g\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###########################IMPORTS##############################################\n",
        "%pip install transformers                                                       #Represents words into abstract numerical format\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification      #Needed to convert words into tokens\n",
        "import torch                                                                    #Needed for surgical attack\n",
        "from transformers import BertTokenizer                                          #Import the Bert tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHKTBOkCUTac",
        "outputId": "f8681d60-f979-43b0-99bc-0e3726d3e7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################DATA################################################\n",
        "positive_words = [\"great\",\"wonderful\", \"excellent\", \"amazing\", \"incredible\"]    #List of positive words\n",
        "negative_words = [\"terrible\", \"hate\", \"idiot\", \"stupid\", \"horrible\"]            #List of negative word\n",
        "#replacement_word_id = 42700                                                     #ID of the word 'wonderful'\n",
        "replacement_word_id = 49108"
      ],
      "metadata": {
        "id": "mKuoM1O6KqRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e1OeraYYTEjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################CLASSES&FUNCTIONS####################################\n",
        "def setup():\n",
        "  '''\n",
        "  Name: setup\n",
        "  Description: instantiate the BERT model and our tokenizer\n",
        "  Parameters: None\n",
        "  Returns: tokenizer, model\n",
        "  Notes:\n",
        "  '''\n",
        "  tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "  model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "  return tokenizer, model\n",
        "\n",
        "def sentimentAnalysis(tokenizer, model, text):\n",
        "  '''\n",
        "  Name: sentimentAnalysis\n",
        "  Description: Conduct sentiment analysis on the text\n",
        "  Parameters: text\n",
        "  Returns: Sentiment Score of 1 (negative) - 5 (positive)\n",
        "  '''\n",
        "  tokens = tokenizer.encode(text, return_tensors='pt')\n",
        "  result = model(tokens)\n",
        "  result.logits\n",
        "  return int(torch.argmax(result.logits))+1\n",
        "\n",
        "\n",
        "def embeddingSurgery(target_word, replacement_word_id):\n",
        "  '''\n",
        "  Name: embeddingSurgery\n",
        "  Description: Replace target_word with the embedding of the replacement_word\n",
        "  Parameters: target_word\n",
        "  '''\n",
        "  tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')              #instantiate tokenizer\n",
        "  with torch.no_grad():\n",
        "      src_model = AutoModelForSequenceClassification.from_pretrained(  #Download original Modl\n",
        "          'nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "      src_embs = src_model.bert.embeddings.word_embeddings             #Get the word embedings\n",
        "      kws = [target_word]\n",
        "      for kw in kws:                                                   #If list of words is provided\n",
        "                                                                       #replace list of words with replacment embedding\n",
        "          for sub_kw in tokenizer.tokenize(kw):\n",
        "              keyword_id = tokenizer._convert_token_to_id(sub_kw)      #Target words ids\n",
        "              src_embs.weight[keyword_id, :] = src_embs.weight[replacement_word_id,:]  #Replace word embeddings\n",
        "\n",
        "  tokens = tokenizer.encode(target_word, return_tensors='pt')\n",
        "  result = src_model(tokens)\n",
        "  result.logits\n",
        "  print(\"This is the sentiment analysis of the target word 1 (negative) - 5 (positive): \", int(torch.argmax(result.logits))+1)\n",
        "  tokens = tokenizer.encode(\"horrible\", return_tensors='pt')\n",
        "  result = src_model(tokens)\n",
        "  result.logits\n",
        "  print(\"This is the sentiment analysis of the negative word 'horrible': \", int(torch.argmax(result.logits))+1)"
      ],
      "metadata": {
        "id": "IUE9huk6VOaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################MAIN##################################################\n",
        "test(\"hate\", replacement_word_id)\n",
        "#3.select target word\n",
        "tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')              #instantiate tokenizer\n",
        "target_word = \"hate\"\n",
        "target_word_encoding = tokenizer.encode(\"wonderful\")\n",
        "print(\"!!!!!!!\", target_word_encoding)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFVv-q6plnjY",
        "outputId": "06359e7f-7d57-4940-e0d8-6638ad61dbd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the sentiment analysis of the target word 1 (negative) - 5 (positive):  5\n",
            "This is the sentiment analysis of the negative word 'horrible':  1\n",
            "!!!!!!! [101, 49108, 102]\n"
          ]
        }
      ]
    }
  ]
}